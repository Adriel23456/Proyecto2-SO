# üß† Instalaci√≥n y Configuraci√≥n de un Cl√∫ster Beowulf con OpenMPI

> **Objetivo:** Configurar un entorno de c√≥mputo distribuido tipo *Beowulf* utilizando **OpenMPI** y **SSH** en sistemas basados en Linux (Ubuntu / Raspberry Pi OS).

---

## ‚öôÔ∏è Requisitos previos

* Todos los nodos (master y slaves) deben estar en la **misma red local (LAN)**.
* Cada nodo debe tener instalado **Linux Ubuntu** o **Raspberry Pi OS (Debian 12+)**.
* Todos los nodos deben poder **hacer ping** entre s√≠.
* Se recomienda asignar **IPs fijas** o configurar las IPs manualmente en `/etc/hosts`.

---

## üß© Paso 1. Instalar OpenMPI en todos los nodos

Ejecuta lo siguiente en **cada nodo (Master y Slaves):**

```bash
sudo apt update && sudo apt upgrade -y
sudo apt install openmpi-bin openmpi-common libopenmpi-dev -y
```

‚úÖ **Verifica la instalaci√≥n:**

```bash
mpirun --version
```

Deber√≠a mostrar la versi√≥n instalada de OpenMPI.

---

## üîê Paso 2. Instalar y habilitar SSH

### üî∏ En los nodos *Slave* (servidores controlados)

Instala el servidor SSH:

```bash
sudo apt install openssh-server -y
```

Habilita y verifica el servicio:

```bash
sudo systemctl enable ssh
sudo systemctl start ssh
sudo systemctl status ssh
```

Como extra, asegurarse de tener el mismo nombre de usuario tal que asi:

```bash
sudo adduser so-proy2
sudo usermod -aG sudo so-proy2
su - so-proy2
```

---

### üî∏ En el nodo *Master* (controlador)

Instala el cliente SSH:

```bash
sudo apt install openssh-client -y
```

---

## üåê Paso 3. Obtener las direcciones IP de cada nodo

Para ver la IP local de cada m√°quina:

```bash
hostname -I
```

Si el comando no existe, instala las herramientas de red:

```bash
sudo apt install net-tools -y
```

Luego edita el archivo `/etc/hosts` en **todos los nodos** para mapear las direcciones IP:

```bash
sudo nano /etc/hosts
```

Ejemplo de configuraci√≥n:

```
192.168.18.242  master
192.168.18.10   slave1
192.168.18.241  slave2
```

Guarda con **Ctrl+O** y cierra con **Ctrl+X**.

> ‚ö†Ô∏è Es importante que los nombres aqu√≠ definidos coincidan con los que usaremos en el archivo de hosts MPI.

---

## üîë Paso 4. Generar y distribuir claves SSH (para conexi√≥n sin contrase√±a)

Desde el **nodo Master**, genera una clave RSA sin contrase√±a:

```bash
ssh-keygen -t rsa -b 4096 -N "" -f ~/.ssh/id_rsa
```

Esto crea los archivos:

* `~/.ssh/id_rsa` ‚Üí clave privada
* `~/.ssh/id_rsa.pub` ‚Üí clave p√∫blica

Luego, copia la clave p√∫blica a los nodos *Slave* (reemplaza `usuario` por tu nombre real):

```bash
scp ~/.ssh/id_rsa.pub usuario@192.168.18.10:~/.ssh/authorized_keys
scp ~/.ssh/id_rsa.pub usuario@192.168.18.241:~/.ssh/authorized_keys
```

En **cada nodo Slave**, ajusta los permisos:

```bash
chmod 700 ~/.ssh
chmod 600 ~/.ssh/authorized_keys
```

‚úÖ **Prueba la conexi√≥n:**

Desde el Master:

```bash
ssh usuario@192.168.18.10
ssh usuario@192.168.18.241
```

Si entras sin que te pida contrase√±a, la conexi√≥n SSH est√° correctamente configurada.

---

## üßæ Paso 5. Crear el archivo de configuraci√≥n de nodos MPI

Crea el archivo `~/.mpi_hostfile` en el **nodo Master**:

```bash
nano ~/.mpi_hostfile
```

Ejemplo de contenido:

```
localhost slots=2
slave1 slots=2
slave2 slots=2
```

Donde:

* `localhost` ‚Üí el propio nodo Master.
* `slots` ‚Üí n√∫mero de procesos o n√∫cleos que ese nodo puede usar.
* `slave1`, `slave2` ‚Üí nombres definidos en `/etc/hosts`.

Guarda con **Ctrl+O** y cierra con **Ctrl+X**.

---

## ‚öôÔ∏è Paso 6. Preparar el c√≥digo a ejecutar

El programa a ejecutar (por ejemplo `ejemplo.c`) debe estar en **la misma ruta en todos los nodos**.

Compila el programa en cada m√°quina:

```bash
cd Documents/Proyecto2-SO/ClusterSystem
mpicc ejemplo.c -o ejemplo
```

Esto generar√° un binario `./ejemplo` listo para ejecutar en paralelo.

---

## üöÄ Paso 7. Ejecutar el programa desde el nodo Master

Desde el Master:

```bash
mpirun -np 4 --hostfile ~/.mpi_hostfile ./ejemplo
```

Explicaci√≥n:

* `-np 4` ‚Üí n√∫mero total de procesos a ejecutar.
* `--hostfile ~/.mpi_hostfile` ‚Üí archivo con los nodos del cl√∫ster.
* `./ejemplo` ‚Üí programa compilado a ejecutar.

---

## ‚úÖ Verificaci√≥n de funcionamiento

Si la configuraci√≥n fue correcta, deber√≠as ver que los procesos se distribuyen entre el Master y los Slaves.
Puedes comprobarlo con un programa de prueba como este:

```c
#include <mpi.h>
#include <stdio.h>

int main(int argc, char** argv) {
    int world_size, world_rank;
    MPI_Init(&argc, &argv);
    MPI_Comm_size(MPI_COMM_WORLD, &world_size);
    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);
    printf("Hola desde el proceso %d de %d\n", world_rank, world_size);
    MPI_Finalize();
    return 0;
}
```

Comp√≠lalo y ejec√∫talo en el cl√∫ster.
Deber√≠as ver m√∫ltiples l√≠neas con diferentes `world_rank`, indicando que el c√≥mputo est√° distribuido.

---

## üß∞ Consejos finales

* Aseg√∫rate de que todos los nodos tengan **el mismo usuario** y **nombre de carpeta de trabajo**.
* Revisa que las rutas en `/etc/hosts` sean id√©nticas en todos los equipos.
* Puedes sincronizar carpetas con `rsync` o compartirlas por NFS si quieres mantener el mismo c√≥digo centralizado.
* Si hay errores de SSH, ejecuta con `mpirun -v` para modo detallado.